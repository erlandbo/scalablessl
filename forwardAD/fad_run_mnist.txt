1
0.0
2.3364067
2
0.125
2.3076315
3
0.0625
2.3171191
4
0.125
2.323245
5
0.0625
2.3358507
6
0.125
2.2955527
7
0.1875
2.2672534
8
0.3125
2.2814953
9
0.0625
2.2696316
10
0.1875
2.2628517
11
0.1875
2.2323136
12
0.125
2.286898
13
0.0625
2.3081393
14
0.3125
2.2459023
15
0.3125
2.2277231
16
0.25
2.2408495
17
0.125
2.2730188
18
0.0625
2.296022
19
0.25
2.2421696
20
0.25
2.220419
21
0.125
2.2594872
22
0.1875
2.2123358
23
0.3125
2.2129855
24
0.125
2.2466826
25
0.0625
2.2441876
26
0.4375
2.2298493
27
0.5625
2.1194181
28
0.5625
2.149815
29
0.375
2.1956482
30
0.375
2.135128
31
0.25
2.164424
32
0.1875
2.2332392
33
0.1875
2.2055697
34
0.375
2.1480255
35
0.375
2.1057925
36
0.3125
2.1863031
37
0.5
2.1169496
38
0.5625
2.1571183
39
0.3125
2.1486754
40
0.4375
2.0961945
41
0.1875
2.1262293
42
0.5
2.000236
43
0.3125
2.1092122
44
0.25
2.1684961
45
0.375
2.141441
46
0.1875
2.1735458
47
0.25
2.1088462
48
0.3125
2.0647204
49
0.4375
1.9956398
50
0.1875
2.1273766
51
0.375
2.004457
52
0.4375
1.9962676
53
0.3125
2.0405872
54
0.1875
2.0870876
55
0.4375
1.9566724
56
0.375
2.105041
57
0.375
2.064592
58
0.625
2.0082312
59
0.5
2.0305896
60
0.6875
1.8658689
61
0.3125
2.0195222
62
0.25
2.088477
63
0.5625
1.9544137
64
0.625
1.8926386
65
0.625
1.8856728
66
0.6875
1.8898757
67
0.5625
1.9457078
68
0.625
1.9259318
69
0.75
1.8510853
70
0.5
2.0023098
71
0.6875
1.8877399
72
0.6875
1.8415097
73
0.75
1.8223718
74
0.4375
1.9321189
75
0.625
1.8691274
76
0.5625
1.7415135
77
0.6875
1.8569026
78
0.6875
1.8079221
79
0.375
1.9846611
80
0.875
1.8150094
81
0.5
1.8699672
82
0.8125
1.74296
83
0.625
1.8062272
84
0.875
1.7148695
85
0.75
1.8274679
86
0.8125
1.6961069
87
0.75
1.5640432
88
0.6875
1.7566361
89
0.875
1.6604328
90
0.8125
1.6706386
91
0.8125
1.5576184
92
0.875
1.6547046
93
0.75
1.6310194
94
0.9375
1.6128919
95
0.75
1.6724601
96
0.6875
1.6262408
97
0.5625
1.7475743
98
0.4375
1.8378667
99
0.5625
1.615026
100
0.6875
1.5925956
2023-09-15 22:35:52.977141: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
{'train_loss': 2.016301079551772, 'test_loss': 1.588178688621521, 'train_acc': 0.4399752475247525, 'test_acc': 0.7129}
1
0.8125
1.3674183
2
0.6875
1.57648
3
0.8125
1.4444582
4
0.625
1.7756886
5
0.75
1.6732594
6
0.8125
1.4257479
7
0.75
1.4893653
